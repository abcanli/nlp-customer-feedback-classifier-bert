NLP Customer Feedback Classifier (BERT + ML Baseline)

This project builds an end-to-end NLP pipeline for classifying **SaaS customer feedback** into four actionable categories:

- `bug_report`
- `feature_request`
- `praise`
- `cancellation_risk`

It includes both a **TF-IDF + Logistic Regression** baseline and a **fine-tuned DistilBERT model**, enabling performance comparison between classical ML and modern transformer-based approaches.

The project is designed as a realistic, production-style ML workflow for Product Analytics, CX teams, and NLP/ML roles.

---

ğŸš€ Key Features

- Synthetic SaaS feedback dataset (balanced across 4 categories)
- Preprocessing pipeline: cleaning, normalization, label encoding, dataset splits
- Baseline model: **TF-IDF + Logistic Regression**
- Transformer model: **DistilBERT fine-tuning** (HuggingFace)
- Evaluation tools:
  - Accuracy, precision, recall, F1-score
  - Confusion matrices & model comparison plots
- Explainability (LIME stub for baseline + SHAP recommendations for BERT)
- Fully structured & reproducible ML project layout
- Works on CPU or GPU

---

ğŸ§± Project Structure

nlp-customer-feedback-classifier-bert/
â”œâ”€â”€ data/

â”‚ â”œâ”€â”€ raw/

â”‚ â”‚ â””â”€â”€ sample_raw_feedback.csv

â”‚ â””â”€â”€ processed/

â”œâ”€â”€ notebooks/

â”‚ â””â”€â”€ 01_eda.ipynb

â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ config.py

â”‚ â”œâ”€â”€ preprocess.py

â”‚â”œâ”€â”€ train_baseline.py

â”‚ â”œâ”€â”€ train_bert.py

â”‚ â”œâ”€â”€ evaluate.py

â”‚ â””â”€â”€ explain.py

â”œâ”€â”€ models/

â”‚ â”œâ”€â”€ baseline/

â”‚ â””â”€â”€ bert/

â”œâ”€â”€ outputs/

â”‚ â”œâ”€â”€ metrics/

â”‚ â”œâ”€â”€ plots/

â”‚ â””â”€â”€ explainability/

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md
ğŸ“Š Dataset

`data/raw/sample_raw_feedback.csv` contains synthetic SaaS feedback examples with:

- `feedback_id`
- `text`
- `label` (`bug_report`, `feature_request`, `praise`, `cancellation_risk`)

This dataset can be easily replaced with real feedback from:
- Zendesk  
- Intercom  
- Freshdesk  
- CRM exports  
- User reports  
- App reviews  

---

ğŸ§ª Models

1ï¸âƒ£ Baseline â€” TF-IDF + Logistic Regression**

- Fast to train  
- Interpretable  
- Strong baseline performance (F1 â‰ˆ 0.85â€“0.90)  
- Artifacts saved under `models/baseline/`

2ï¸âƒ£ DistilBERT â€” Fine-Tuned Transformer**

- Captures contextual meaning  
- Handles complex phrasing  
- Typically higher accuracy (F1 â‰ˆ 0.90â€“0.95)  
- Trained via HuggingFace Transformers  
- Saved under `models/bert/`

---

âš™ï¸ Setup & Installation

```bash
git clone https://github.com/abcanli/nlp-customer-feedback-classifier-bert.git
cd nlp-customer-feedback-classifier-bert
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

â–¶ï¸ How to Run the Pipeline

1. Preprocess Dataset
python src/preprocess.py

2. Train Baseline Model
python src/train_baseline.py

3. Train BERT Model
python src/train_bert.py

4. Evaluate Models
python src/evaluate.py
Outputs include:
Confusion matrices
Comparison plots
Classification reports

5. Explainability
python src/explain.py

ğŸ“ˆ Example Performance (Typical)
Model	F1 Score	Notes
TF-IDF + Logistic Regression	0.85â€“0.90	Fast & simple
DistilBERT	0.90â€“0.95	Best performance

ğŸ§© Extend This Project
Add FastAPI inference endpoint
Add SHAP explainability for BERT
Deploy model as a microservice
Build a Streamlit dashboard for predictions
Use a real SaaS feedback dataset

ğŸ‘¤ Author
Ali Berk CanlÄ±
NLP/ML Analyst â€¢ Data Product Analyst
LinkedIn: https://www.linkedin.com/in/aliberkcanlÄ±
GitHub: https://github.com/abcanli

### ğŸ” Visual Comparison

![Model comparison](outputs/plots/model_comparison.png)
